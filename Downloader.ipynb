{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import socket\n",
    "import socks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_title(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "            jobs.append(a[\"title\"])\n",
    "    return(jobs)\n",
    "\n",
    "def get_company(soup): \n",
    "    companies = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "    else:\n",
    "        sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "        for span in sec_try:\n",
    "            companies.append(span.text.strip())\n",
    "    return(companies)\n",
    "\n",
    "def get_location(soup): \n",
    "    locations = []\n",
    "    spans = soup.findAll(\"span\", attrs={\"class\": \"location\"})\n",
    "    for span in spans:\n",
    "        locations.append(span.text)\n",
    "    return(locations)\n",
    "\n",
    "def get_posting_date(soup):\n",
    "    dates = []\n",
    "    for i in soup.findAll(\"span\", attrs={\"class\": \"date\"}):\n",
    "        dates.append(i.text)\n",
    "    return dates\n",
    "\n",
    "def get_rating(soup):\n",
    "    tag = soup.findAll(\"meta\", attrs={\"itemprop\": \"ratingValue\"})\n",
    "    if len(tag) == 0: \n",
    "        rating = 0\n",
    "    else:\n",
    "        rating = tag[0].get('content')\n",
    "    return rating\n",
    "\n",
    "def get_links(soup):\n",
    "    links = []\n",
    "    for i in soup.find_all(name=\"a\", attrs={\"target\":\"_blank\"}):\n",
    "        href = i.get('href')\n",
    "        if '/rc/clk' in href:\n",
    "            links.append(href[7:])\n",
    "    return links\n",
    "\n",
    "def get_soup(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    else:\n",
    "        print('Status code is not 200')\n",
    "        soup = 0\n",
    "        time.sleep(120)\n",
    "    return soup\n",
    "\n",
    "def get_job_descr(soup):\n",
    "    descr = ''\n",
    "\n",
    "        \n",
    "    for i in soup.find_all(name='ul'):\n",
    "        descr += ' ' + (str(i.text))\n",
    "    descr = descr.replace('\\n',' ')\n",
    "    return descr\n",
    "\n",
    "def get_other_text(soup):\n",
    "    text = ''\n",
    "    for i in soup.find_all(name='p'):\n",
    "        text += ' ' + (str(i.text))\n",
    "\n",
    "    for i in soup.find_all(name='li'):\n",
    "        text += ' ' + (str(i.text))\n",
    "    \n",
    "    for i in soup.find_all(name='br'):\n",
    "        text += ' ' + (str(i.text))\n",
    "        \n",
    "    text = text.replace('\\n',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.indeed.com/jobs?q=data+scientist&sort=date\"\n",
    "# cities = {'New+York%2C+NY':320,'San+Francisco%2C+CA':380,'Washington%2C+DC':540,'Texas':220,'Seattle%2C+WA':300,\n",
    "#          'Los+Angeles%2C+CA':120,'Chicago%2C+IL':120}\n",
    "\n",
    "cities = {'https://www.indeed.com/jobs?q=Data+Scientist&sort=date&start=' : 10,\n",
    "          'https://www.indeed.co.uk/jobs?q=data+scientist&l=London&sort=date&start=' : 0}\n",
    "\n",
    "\n",
    "# socks.set_default_proxy(socks.SOCKS5,'localhost',9150)\n",
    "# socket.socket=socks.socksocket\n",
    "\n",
    "data = pd.DataFrame(columns=['title', 'posting_date', 'company_rating', \n",
    "                             'company', 'location', 'description', 'text', 'url'])\n",
    "\n",
    "for c in cities:\n",
    "    for count in range(0,cities[c]+1,10):\n",
    "#         url = f'https://www.indeed.com/jobs?q=data+scientist&l={c}&sort=date&start={count}'\n",
    "        url = c+str(cities[c])\n",
    "        soup = get_soup(url)\n",
    "        if soup ==0:\n",
    "            continue\n",
    "        links = get_links(soup)\n",
    "        df = pd.DataFrame(columns=data.columns)\n",
    "        df['title'] = get_job_title(soup)\n",
    "        df['company'] = get_company(soup)\n",
    "        df['location'] = get_location(soup)\n",
    "        df['posting_date'] = get_posting_date(soup)\n",
    "        for i,l in enumerate(links):\n",
    "            link = 'https://www.indeed.com/viewjob' +l\n",
    "            df.loc[i]['url'] = link\n",
    "            print(link)\n",
    "            page = get_soup(link)\n",
    "            if soup == 0:\n",
    "                continue\n",
    "            df.loc[i]['description'] = get_job_descr(page)\n",
    "            df.loc[i]['text'] = get_other_text(page)\n",
    "            df.loc[i]['company_rating'] = get_rating(page)\n",
    "            time.sleep(randint(3,5))\n",
    "        data = pd.concat([data,df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clear(x):\n",
    "    trash = ' Hiring LabCareer AdviceBrowse JobsBrowse CompaniesSalariesFind CertificationsEmployer EventsWork at IndeedCountriesAboutHelp Center Do Not Sell My Personal InformationPrivacy Center'\n",
    "    if trash in x:\n",
    "        x.replace(trash,'')\n",
    "    return x\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data['description'] = data.description.map(lambda x: clear(x))\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data.posting_date=='1 day ago')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('new.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
