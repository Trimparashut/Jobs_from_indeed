{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_bank.csv')\n",
    "df.drop(['posting_date','company_rating','company','location','url'],inplace = True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    text = re.sub('[.,:;_%Â©?*,!@#$%^&()]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    d = {'1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'}\n",
    "    text = ''.join([d.get(word, word) for word in text])\n",
    "    text = ' '.join(word for word in text.split() if len(word)>1)\n",
    "    return text\n",
    "\n",
    "def get_prepared_text(text):\n",
    "    text = clean_text(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    without_stop_words = ''\n",
    "    for word in words:\n",
    "        if word not in stop_words and re.search('[a-zA-Z]',word):\n",
    "            without_stop_words += word + ' '\n",
    "    return without_stop_words\n",
    "\n",
    "def get_vectorizer(df,min_df, max_df, ngram_range):\n",
    "  tfidf_vectorizer = TfidfVectorizer(min_df=min_df, max_df = max_df, ngram_range=ngram_range,stop_words=stop_words,use_idf = True)\n",
    "  msg = []\n",
    "  for i, row in df.iterrows():\n",
    "    line = row[df.columns[0]].lower().strip()\n",
    "    msg.append(line)\n",
    "  tfidf_vectorizer.fit(msg) \n",
    "  return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "to_stop = pd.DataFrame(df['text'])\n",
    "to_stop['text'] = to_stop['text'].map(lambda x: get_prepared_text(x))\n",
    "tfidf_vectorizer = get_vectorizer(to_stop,min_df=0.3, max_df = 1.0, ngram_range=(1,1))\n",
    "s = tfidf_vectorizer.get_feature_names()\n",
    "for i in s: \n",
    "  stop_words.add(i)\n",
    "print('Words added to stoplist: '+str(len(s))) \n",
    "\n",
    "data = pd.DataFrame(df['description'])\n",
    "data['description'] = data.description.map(lambda x: get_prepared_text(x))\n",
    "tfidf_vectorizer = get_vectorizer(data,min_df=0.01, max_df=0.5, ngram_range=(1,1))\n",
    "msg = []\n",
    "for i, row in data.iterrows():\n",
    "    line = row[data.columns[0]].lower().strip()\n",
    "    msg.append(line)\n",
    "tfidf_matrix = tfidf_vectorizer.transform(msg)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix.toarray(), \n",
    "    df['target'].values, \n",
    "    test_size=0.2, \n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'loss_function' : ['CrossEntropy','LogLoss'],\n",
    "    'eval_metric' : ['F1','Recall','Accuracy'],\n",
    "    'max_depth' : [6,8,10],\n",
    "    'boosting_type' : ['Ordered'],\n",
    "    'num_trees' : [750,1000],\n",
    "    'learning_rate' : [0.003,0.005,0.007],\n",
    "}\n",
    "\n",
    "from itertools import product\n",
    "all_params = [dict(zip(grid, v)) for v in product(*grid.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# sample = random.sample(all_params,150)\n",
    "results = []\n",
    "for params in tqdm(all_params):\n",
    "  loss_function = params['loss_function']\n",
    "  eval_metric = params['eval_metric']\n",
    "  max_depth = params['max_depth']\n",
    "  boosting_type = params['boosting_type']\n",
    "  num_trees = params['num_trees']\n",
    "  learning_rate = params['learning_rate']\n",
    "\n",
    "  print(params)\n",
    "  model = CatBoostClassifier(verbose=False, \n",
    "                             loss_function=loss_function, \n",
    "                             eval_metric=eval_metric,\n",
    "                             max_depth=max_depth, \n",
    "                             boosting_type=boosting_type, \n",
    "                             num_trees=num_trees,\n",
    "                             learning_rate=learning_rate,\n",
    "                             task_type=\"GPU\")\n",
    "  model.fit(X_train,y_train)\n",
    "  pred = model.predict(X_test)\n",
    "\n",
    "  acc = accuracy_score(pred,y_test)\n",
    "  rec = recall_score(pred,y_test)\n",
    "  prec = precision_score(pred,y_test)\n",
    "  f1 = f1_score(pred,y_test)\n",
    "  \n",
    "  results.append([acc,rec,prec,f1])\n",
    "  print()\n",
    "  print('Accuracy ', acc)\n",
    "  print('Recall ', rec)\n",
    "  print('Precision ', prec)\n",
    "  print('F1 ', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['acc','rec','prec','f1'], data = results)\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "  plt.figure(figsize=(16, 6))\n",
    "  plt.plot(res.index, res.acc, label='acc')\n",
    "  plt.plot(res.index, res.rec, label='rec')\n",
    "  plt.plot(res.index, res.prec, label='prec')\n",
    "  plt.plot(res.index, res.f1, label='f1')\n",
    "  plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(loss_function='CrossEntropy', eval_metric= 'Accuracy', max_depth =  10, \n",
    "                           boosting_type = 'Ordered', num_trees = 850, learning_rate = 0.003,task_type=\"GPU\")\n",
    "model.fit(tfidf_matrix.toarray(),df['target'].values)\n",
    "\n",
    "import pickle\n",
    "filename = 'model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "files.download(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
